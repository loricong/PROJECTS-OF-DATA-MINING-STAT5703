# Packages for data dimension reduction
install.packages("data.table")
install.packages("factoextra")
install.packages("FactoMineR")
install.packages("corrplot")
install.packages("ggplot2")
install.packages("plyr")
install.packages("dplyr")
install.packages("PerformanceAnalytics")

install.packages("tidyverse")
install.packages("ipred")
install.packages("caret")
install.packages("lattice") 
install.packages("rpart")
install.packages("rpart.plot")



library("data.table")
library("factoextra")
library("FactoMineR")
library("corrplot")
library("ggplot2")
library(plyr)
library("dplyr")
library("PerformanceAnalytics")

library("tidyverse")
library("lattice") 
library("ipred")
library("caret")
library("rpart")
library("rpart.plot")
library("RColorBrewer")

# Data cleaning 
# Read Audit data in R
audit <- read.csv("~/Desktop/GRADUATE/5703/final project/Data/audit.csv", stringsAsFactors=FALSE)
View(audit)
# Make ID as column ID
rownames(audit) <- audit$ID
audit$ID <- NULL
#Set 'None' for missing values. 
audit[is.na(audit)]<-'None'

# Review variables and clean/re-organize data 
summary(audit$Age)

table(audit$Employment)
# Combined unemployed, volunteer and missings with self-emplyed and others (79+1+1+100)
audit$Employment <- gsub('^SelfEmp', 'SelfEmp_Other', audit$Employment)
audit$Employment <- gsub('^Unemployed', 'SelfEmp_Other', audit$Employment)
audit$Employment <- gsub('^Volunteer', 'SelfEmp_Other', audit$Employment) 
audit$Employment <- gsub('^None', 'SelfEmp_Other', audit$Employment) 
# Also combined all levels of goverment employees
audit$Employment <- gsub('^PSFederal', 'Government', audit$Employment)
audit$Employment <- gsub('^PSLocal', 'Government', audit$Employment)
audit$Employment <- gsub('^PSState', 'Government', audit$Employment) 

table(audit$Education)
# Re-organize education 
# Combined year12 and before to 'Pre-HS'
audit$Education <- gsub('^Yr12', 'Pre_HS', audit$Education)
audit$Education <- gsub('^Yr11', 'Pre_HS', audit$Education)
audit$Education <- gsub('^Yr10', 'Pre_HS', audit$Education)
audit$Education <- gsub('^Yr9', 'Pre_HS', audit$Education) 
audit$Education <- gsub('^Yr7t8', 'Pre_HS', audit$Education) 
audit$Education <- gsub('^Yr5t6', 'Pre_HS', audit$Education) 
audit$Education <- gsub('^Yr1t4', 'Pre_HS', audit$Education) 
audit$Education <- gsub('^Preschool', 'Pre_HS', audit$Education) 
# Combined Associate, Vocational to 'College'
audit$Education <- gsub('^Associate', 'College', audit$Education)
audit$Education <- gsub('^Vocational', 'College', audit$Education)
audit$Education <- gsub('^Professional', 'College', audit$Education)
# Combined 'Doctorate', 'Master' to 'Bachelor'
audit$Education <- gsub('^Doctorate', 'Bachelor', audit$Education)
audit$Education <- gsub('^Master', 'Bachelor', audit$Education)

table(audit$Marital)
# Combined 'Married-spouse-absent' to 'Married'
audit$Marital <- gsub('^Married-spouse-absent', 'Married', audit$Marital)

table(audit$Occupation)
# regroup occupation to White-Collar, White-Collar,Service and Other/Unknown
audit$Occupation <- gsub('^Clerical', 'White_Collar', audit$Occupation)
audit$Occupation <- gsub('^Cleaner', 'Blue_Collar', audit$Occupation)
audit$Occupation <- gsub('^Executive', 'White_Collar', audit$Occupation)
audit$Occupation <- gsub('^Farming', 'Blue_Collar', audit$Occupation)
audit$Occupation <- gsub('^Machinist', 'Blue_Collar', audit$Occupation)
audit$Occupation <- gsub('^Protective', 'Service', audit$Occupation)
audit$Occupation <- gsub('^Sales', 'Service', audit$Occupation)
audit$Occupation <- gsub('^Support', 'Service', audit$Occupation)
audit$Occupation <- gsub('^Transport', 'Service', audit$Occupation)
audit$Occupation <- gsub('^Repair', 'Blue_Collar', audit$Occupation)
audit$Occupation <- gsub('^Military', 'Other_Unknown', audit$Occupation)
audit$Occupation <- gsub('^Home', 'Other_Unknown', audit$Occupation)
audit$Occupation <- gsub('^None', 'Other_Unknown', audit$Occupation)

summary(audit$Income)
table(audit$Gender)
summary(audit$Deductions)
summary(audit$Hours)
summary(audit$RISK_Adjustment)
table(audit$TARGET_Adjusted)


write.csv(audit,'audit_cleaned.csv')
#Also Use Ggobi generate the scatter plot for variables and PCs
# Scatter Matrix of "SactterPlot_Varibles_PC1_PC9" is generated by Ggobi and saved
# scatterplots
install.packages("corrgram")
library(corrgram)
panel.cor <- function(x, y, digits=2, prefix="", cex.cor) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(0, 1, 0, 1))
  r <- abs(cor(x, y))
  txt <- format(c(r, 0.123456789), digits=digits)[1]
  txt <- paste(prefix, txt, sep="")
  if(missing(cex.cor)) cex.cor <- 0.8/strwidth(txt)*r
  text(0.5, 0.5, txt, cex = cex.cor)
}

panel.hist <- function(x, ...) {
  usr <- par("usr"); on.exit(par(usr))
  par(usr = c(usr[1:2], 0, 1.5) )
  h <- hist(x, plot = FALSE)
  breaks <- h$breaks; nB <- length(breaks)
  y <- h$counts; y <- y/max(y)
  rect(breaks[-nB], 0, breaks[-1], y, col="cyan", ...)
}
#scatter plots
jpeg('scatter plots.jpeg')
pairs(audit[,c("Age","Income","Deductions","Hours")],
      upper.panel = panel.cor, diag.panel = panel.hist, cex.labels = .93)
dev.off()
# correlogram and correlation coefficients
jpeg('correlogram and correlation coefficients.jpeg')
corrgram(audit[,c("Age","Income","Deductions","Hours")],
         lower.panel=panel.shade, upper.panel=panel.pie,
         diag.panel=panel.minmax, cex.labels = .95)
dev.off()
round(cor(audit[,c("Age","Income","Deductions","Hours")]), digits = 2)

###### AGE #####
# histogram of age by TARGET_adj group
jpeg('histogram of age by TARGET_adj group.jpeg')
ggplot(audit) + aes(x=as.numeric(Age), group=TARGET_Adjusted, fill=factor(TARGET_Adjusted)) + 
  geom_histogram(binwidth=1, color='black')
dev.off()
#majority of the observations have 0-target_adjusted (nonproductive audit)

###### Employment ######
table(audit$Employment)
#explore the relationship between Employment and TARGET_Adjusted
#barplot of Employment by TARGET_Adjusted group
# get the counts by employment and target_adj group
count <- table(audit[audit$Employment == 'Consultant',]$TARGET_Adjusted)["0"]
count <- c(count, table(audit[audit$Employment == 'Consultant',]$TARGET_Adjusted)["1"])
count <- c(count, table(audit[audit$Employment == 'Government',]$TARGET_Adjusted)["0"])
count <- c(count, table(audit[audit$Employment == 'Government',]$TARGET_Adjusted)["1"])
count <- c(count, table(audit[audit$Employment == 'Private',]$TARGET_Adjusted)["0"])
count <- c(count, table(audit[audit$Employment == 'Private',]$TARGET_Adjusted)["1"])
count <- c(count, table(audit[audit$Employment == 'SelfEmp_Other',]$TARGET_Adjusted)["0"])
count <- c(count, table(audit[audit$Employment == 'SelfEmp_Other',]$TARGET_Adjusted)["1"])
count
count <- as.numeric(count)
# create a dataframe
audit$Employment<-as.factor(audit$Employment)
Employment <- rep(levels(audit$Employment), each = 2)
TARGET_Adjusted <- rep(c('0', '1'), 4)
df <- data.frame(Employment, TARGET_Adjusted, count)
df
# calculate the percentages
df <- ddply(df, .(Employment), transform, percent = count/sum(count) * 100)
# format the labels and calculate their positions
df <- ddply(df, .(Employment), transform, pos = (cumsum(count) - 0.5 * count))
df$label <- paste0(sprintf("%.0f", df$percent), "%")
# bar plot of counts by employment with in group proportions 
jpeg('histogram of Employment by Target_Adjusted.jpeg')
ggplot(df, aes(x = Employment, y = count, fill =TARGET_Adjusted )) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = pos, label = label), position = position_stack(vjust = 0.1), size = 2.5) + 
  ggtitle('TARGET_Adjusted with Employment')
dev.off()
#there's no significant difference between employment, all around 20%


######## Education ###########
# create a dataframe
audit$Education<-as.factor(audit$Education)
df1 <- data.frame(table(audit$TARGET_Adjusted, audit$Education))
names(df1) <- c('TARGET_Adjusted', 'Education', 'count')
df1
# calculate the percentages
df1 <- ddply(df1, .(Education), transform, percent = count/sum(count) * 100)
# format the labels and calculate their positions
df1 <- ddply(df1, .(Education), transform, pos = (cumsum(count) - 0.5 * count))
df1$label <- paste0(sprintf("%.0f", df1$percent), "%")
# bar plot of counts by education with in group proportions 
jpeg('histogram of Education by TARGET_Adjusted.jpeg')
ggplot(df1, aes(x = Education, y = count, fill = TARGET_Adjusted)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = pos, label = label), position = position_stack(vjust = 0.1),size = 2.5) + 
  ggtitle('TARGET_Adjusted with Education')
dev.off()
#in group proportion of having productive audit increase as the level of education increases. 
#For those who donâ€™t have any forms of college education, less than 20% have a productive audit. 

######## marital #######
audit$Marital<-as.factor(audit$Marital)
table(audit$Marital)
df2 <- data.frame(table(audit$TARGET_Adjusted, audit$Marital))
names(df2) <- c('TARGET_Adjusted', 'Marital', 'count')
df2
# calculate the percentages
df2 <- ddply(df2, .(Marital), transform, percent = count/sum(count) * 100)
# format the labels and calculate their positions
df2 <- ddply(df2, .(Marital), transform, pos = (cumsum(count) - 0.5 * count))
df2$label <- paste0(sprintf("%.0f", df2$percent), "%")
# bar plot of counts by marital status with in group proportions 
jpeg('histogram of Marital by TARGET_Adjusted.jpeg')
ggplot(df2, aes(x = Marital, y = count, fill = TARGET_Adjusted)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = pos, label = label), position = position_stack(vjust = 0.1),size = 2.5) + 
  ggtitle('TARGET_Adjusted with Marital Status')
dev.off()

######## occupation #######
audit$Occupation<-as.factor(audit$Occupation)
table(audit$Occupation)
df3 <- data.frame(table(audit$TARGET_Adjusted, audit$Occupation))
names(df3) <- c('TARGET_Adjusted', 'Occupation', 'count')
df3
# calculate the percentages
df3 <- ddply(df3, .(Occupation), transform, percent = count/sum(count) * 100)
# format the labels and calculate their positions
df3 <- ddply(df3, .(Occupation), transform, pos = (cumsum(count) - 0.5 * count))
df3$label <- paste0(sprintf("%.0f", df3$percent), "%")
# bar plot of counts by Occupation with in group proportions 
jpeg('histogram of Occupation by TARGET_Adjusted.jpeg')
ggplot(df3, aes(x = Occupation, y = count, fill = TARGET_Adjusted)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = pos, label = label), position = position_stack(vjust = 0.1),size = 2.5) + 
  ggtitle('TARGET_Adjusted with Occupation')
dev.off()

###### GENDER #####
table(audit$Gender)
df4 <- data.frame(table(audit$TARGET_Adjusted, audit$Gender))
names(df4) <- c('TARGET_Adjusted', 'Gender', 'count')
df4
# calculate the percentages
df4 <- ddply(df4, .(Gender), transform, percent = count/sum(count) * 100)
# format the labels and calculate their positions
df4 <- ddply(df4, .(Gender), transform, pos = (cumsum(count) - 0.5 * count))
df4$label <- paste0(sprintf("%.0f", df4$percent), "%")
# bar plot of counts by Gender with in group proportions 
jpeg('histogram of Gender by TARGET_Adjusted.jpeg')
ggplot(df4, aes(x = Gender, y = count, fill = TARGET_Adjusted)) +
  geom_bar(stat = "identity") +
  geom_text(aes(y = pos, label = label), position = position_stack(vjust = 0.1),size = 2.5) + 
  ggtitle('TARGET_Adjusted with Gender')
dev.off()

################################################
# Target variables should not be included for dimension reduction and data reduction
targetvars<- c("RISK_Adjustment", "TARGET_Adjusted")  
audit.pca <- audit[, ! names(audit) %in% targetvars, drop = F]
# Transfer categorical variables to numeric variables for coorelation matrix
audit.pca.num <-data.matrix(data.frame(unclass(audit.pca))) 

# Generate plot of "ScatterPlot_Matrix_Correlation_Audit.pdf"
pdf("ScatterPlot_Matrix_Correlation_Audit.pdf")
chart.Correlation(audit.pca.num, histogram=TRUE, pch=19)
dev.off() 

#Perfrom  Factor analysis of mixed data (FAMD)
res.famd <- FAMD (audit.pca, ncp = 10, graph = FALSE)
eig.val <- get_eigenvalue(res.famd)
head(eig.val)
out.famd.summary <- capture.output(eig.val)
write.csv (eig.val,file = "FAMD_Summary.csv" )

pdf("Percentage of explained Variance for FAMD.pdf")
fviz_screeplot(res.famd)
dev.off() 
var <- get_famd_var(res.famd)
var
# Coordinates of variables
head(var$coord,10)
write.csv (head(var$coord,10), file = "FAMD_Coordinates_of_variables.csv" ) 

# Cos2: quality of representation on the factore map
head(var$cos2,10)
write.csv (head(var$cos2,10),file = "FAMD_Quality_of_Representation_of_variables.csv" ) 

# Contributions to the dimensions
head(var$contrib,10)
write.csv (head(var$contrib,10),file = "FAMD_Contributions_of_variables.csv" )

# Factor Analysis Plot of Variables
pdf("Factor Analysis Plot of Variables.pdf")
fviz_famd_var(res.famd, repel = TRUE)
dev.off() 

# Contribution to the first dimension
pdf("Contribution to the first dimension.pdf")
fviz_contrib(res.famd, "var", axes = 1)
dev.off() 

# Contribution to the second dimension
pdf("Contribution to the second dimension.pdf")
fviz_contrib(res.famd, "var", axes = 2)
dev.off() 

# Use Decision Tree to perfrom dimension reduction - CART Model 
excldvars<- c("TARGET_Adjusted")  
audit.dt.target <- audit[, ! names(audit) %in% excldvars, drop = F]


# Fit regression tree on "RISK_Adjustment"
anova.model <- rpart(RISK_Adjustment ~ Age+Employment+Education+Marital+Occupation+Income+Gender+Deductions+Hours , 
                     data=audit.dt.target, control = rpart.control(cp = 0.0001), method="anova")

# Prediction error rate in training data = Root node error * rel error * 100%
# Prediction error rate in cross-validation = Root node error * xerror * 100%
# Hence we want the cp value (with a simpler tree) that minimizes the xerror. 

printcp(anova.model)
out.anovamodel.cp <- capture.output(printcp(anova.model))
write.csv (out.anovamodel.cp,file = "Unpruned Regression Tree CP value for RISK_Adjustment.csv" )

pdf("Unpruned Regression Tree for RISK_Adjustment.pdf")
rpart.plot(anova.model)
dev.off() 

bestcp.anova <- anova.model$cptable[which.min(anova.model$cptable[,"xerror"]),"CP"]
(bestcp.anova)
pruned.anova.model <- prune(anova.model, cp = bestcp.anova)

pdf("Pruned Regression Tree for RISK_Adjustment.pdf")
rpart.plot(pruned.anova.model)
dev.off() 

# Conditional inference tree on RISK_Adjustment
install.packages("party")
library(party)
set.seed(123)
ctreemodel <- train(
  RISK_Adjustment ~ Age+Employment+Education+Marital+Occupation+Income+Gender+Deductions+Hours,
  data=audit.dt.target, 
  method = "ctree2",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(maxdepth = 3, mincriterion = 0.95 )
)

pdf("Conditional inference tree for RISK_Adjustment.pdf")
plot(ctreemodel$finalModel)
dev.off() 

#----Clssification regression tree on "TARGET_Adjusted"

excldvars<- c("RISK_Adjustment")  
audit.dt.risk <- audit[, ! names(audit) %in% excldvars, drop = F]

# Fit Classification trees on "TARGET_Adjusted"
class.model <- rpart(TARGET_Adjusted ~ Age+Employment+Education+Marital+Occupation+Income+Gender+Deductions+Hours , 
                     data=audit.dt.risk, control = rpart.control(cp = 0.0001), method="class")

# Prediction error rate in training data = Root node error * rel error * 100%
# Prediction error rate in cross-validation = Root node error * xerror * 100%
# Hence we want the cp value (with a simpler tree) that minimizes the xerror. 

printcp(class.model)
out.classmodel.cp <- capture.output(printcp(class.model))
write.csv (out.classmodel.cp,file = "Unpruned Classification Tree CP value for TARGET_Adjusted.csv" )

pdf("Unpruned Classification Tree for TARGET_Adjusted.pdf")
rpart.plot(class.model)
dev.off() 

bestcp.classificaion <- class.model$cptable[which.min(class.model$cptable[,"xerror"]),"CP"]
(bestcp.classificaion)
pruned.class.model <- prune(class.model, cp = bestcp.classificaion)

pdf("Pruned Classification Tree for TARGET_Adjusted.pdf")
rpart.plot(pruned.class.model,faclen = 0, cex = 0.7, extra = 1)
dev.off() 

pruned.class.model$variable.importance
pruned.class.model.variable.importance <- capture.output(print(pruned.class.model$variable.importance))
write.table(pruned.class.model.variable.importance, file =  "Variable Importance Value of Classification Tree.txt", sep = "\t",
            row.names = TRUE, col.names = NA)

# confusion matrix (training data)
xtab <- table( predict(pruned.class.model,type="class"), audit.dt.risk$TARGET_Adjusted )
confusionMatrix(xtab)
cMatrix <- capture.output(confusionMatrix(xtab)) 
write.table(capture.output(cMatrix) , file = "Confusion Matrix of Classification Tree.txt" , sep = "\t")

print(xtab)

rownames(xtab) <- paste("Pred", rownames(xtab), sep = ":")
colnames(xtab) <- paste("Actual", colnames(xtab), sep = ":")
print(xtab)

# Conditional inference tree on TARGET_Adjusted
library(party)
set.seed(123)
ctreemodel2 <- train(
  TARGET_Adjusted ~ Age+Employment+Education+Marital+Occupation+Income+Gender+Deductions+Hours,
  data=audit.dt.risk, 
  method = "ctree2",
  trControl = trainControl("cv", number = 10),
  tuneGrid = expand.grid(maxdepth = 3, mincriterion = 0.95 )
)

pdf("Conditional inference tree for TARGET_Adjusted.pdf")
plot(ctreemodel2$finalModel)
dev.off() 

# Packages for data data reduction - clustering
install.packages("dplyr") 
install.packages("plyr") 
install.packages("ggplot2")
install.packages("tidyr")
install.packages("factoextra")
install.packages("NbClust")
install.packages("cluster")
install.packages("magrittr")
install.packages("reshape2")
install.packages("stringi")
install.packages("clValid")
install.packages("kohonen")
install.packages("tables")
install.packages("pvclust")
install.packages("descr")
install.packages("stats")
install.packages("statsr")

library("dplyr")
library("plyr")
library("ggplot2")
library("factoextra")
library("tidyr")
library("NbClust")
library("cluster")
library("magrittr")
library("reshape2") 
library("stringi") 
library("clValid")
library("kohonen")
library("tables")
library("pvclust")
library("descr")
library("stats")
library("statsr")
#Based on decision tree analysis, the clsutering analysis exludes
#  employment
cols.dont.want <- c("Employment")  
audit.cluster <- audit[, ! names(audit) %in% cols.dont.want, drop = F]
# Transfer character in categorical variables to numeric values
audit.cluster.num <-data.matrix(data.frame(unclass(audit.cluster))) 
# Excude NA in the data. Indeed, there is no NAs in data file
audit.cluster.num <- audit.cluster.num %>%
  na.omit() 

#Data Reduction - Clustering
#Since there are categorical varibles in the data, 
#Returns the distance matrix with Gower's distance:
audit.gower <- daisy(audit.cluster.num,metric="gower" )

res <- get_clust_tendency(as.matrix(audit.gower), 40, graph = TRUE)
# Hopskin statistic
res$hopkins_stat
write.table(res$hopkins_stat, file = "Hopskin statistic for Audit file.txt" , sep = "\t")

# Since the Hopskin statistic is only 0.0514603, which means it is not a
# uniform data.

# Test Agglomerative Hierarchical Clustering
# with difference methods (dendrograms)
hc.w <- hclust(audit.gower, method = "ward.D")
hc.w2 <- hclust(audit.gower, method = "ward.D2")
hc.s <- hclust(audit.gower, method = "single")
hc.c <- hclust(audit.gower, method = "complete")
hc.a <- hclust(audit.gower, method = "average")
hc.mc <- hclust(audit.gower, method = "mcquitty")
hc.me <- hclust(audit.gower, method = "median")

pdf("Res_Wdd.pdf") 
fviz_dend(hc.w, cex = 0.5)
dev.off() 
pdf("Res_Wdd2.pdf") 
fviz_dend(hc.w2, cex = 0.5)
dev.off() 
pdf("Res_Single.pdf") 
fviz_dend(hc.s, cex = 0.5)
dev.off() 
pdf("Res_Complete.pdf") 
fviz_dend(hc.c, cex = 0.5)
dev.off() 
pdf("Res_Average.pdf") 
fviz_dend(hc.a, cex = 0.5)
dev.off() 
pdf("Res_Mcquitty.pdf") 
fviz_dend(hc.mc, cex = 0.5)
dev.off() 
pdf("Res_Median.pdf") 
fviz_dend(hc.me, cex = 0.5)
dev.off() 

# Divisive Hierarchical Clustering
# compute divisive hierarchical clustering
divisive.clust <- diana(as.matrix(audit.gower), 
                        diss = TRUE, keep.diss = TRUE)
pdf("Divisive.pdf") 
plot(divisive.clust, main = "Divisive")
dev.off()  

# Find optimal number of clusters
# Elbow method
pdf("Elbow_Method.pdf") 
fviz_nbclust(as.matrix(audit.gower), FUN = hcut, method = "wss") +
  geom_vline(xintercept = 4, linetype = 2)+
  labs(subtitle = "Elbow method")
dev.off() 

# Silhouette method
pdf("Silhouette_Method.pdf") 
fviz_nbclust(as.matrix(audit.gower), FUN = hcut, method = "silhouette")+
  labs(subtitle = "Silhouette method")
dev.off()

# Gap statistic
pdf("Gap statistic.pdf") 
set.seed(123)
fviz_nbclust(as.matrix(audit.gower), FUN = hcut, nstart = 25,  method = "gap_stat", nboot = 50)+
  labs(subtitle = "Gap statistic method") 
dev.off()

# Based on a comprehensive evaluation, Hierarchical Clustering
# with Ward's method for 4 clusters will be applied
audit.final <- hclust(audit.gower, method = "ward.D2" )

pdf("Final Hierarchical Clustering with 4 cluster.pdf")
plot(audit.final, cex = 0.6)
rect.hclust(audit.final, k = 4, border = 2:5)
dev.off()

# Cut tree into 4 groups
Cluster <- cutree(audit.final, k = 4)

# Number of members in each cluster
table(Cluster)
## Cluster
##   1   2   3   4 
## 564 568 463 405 

# add cluster in the data
audit.original.cluster <- cbind(audit,Cluster)

#Summary of each cluster

# Select continious variables
Continious.v <-c("Age", "Income","Deductions","Hours","RISK_Adjustment","Cluster")
Categorical.v <-c("Employment","Education","Occupation", "Marital","Gender","TARGET_Adjusted","Cluster"  )

Audit.cluster.Cont <- audit.original.cluster[, names(audit.original.cluster ) %in% Continious.v, drop = F]
Audit.cluster.Cate <- audit.original.cluster[, names(audit.original.cluster ) %in% Categorical.v, drop = F]


mean.audit.cluster <- Audit.cluster.Cont  %>%
  group_by(Cluster)  %>%
  summarise_all(funs(mean)) 

write.csv(mean.audit.cluster, file = "Average values of continious variable in each cluster.csv")

source("http://pcwww.liv.ac.uk/~william/R/crosstab.r")

Table1 <- crosstab(Audit.cluster.Cate, row.vars = "Cluster", col.vars = "Education", type = c("f", "r"), style = "wide", 
                   addmargins = FALSE)
T1.summary <- capture.output(Table1)  
cat("Cluster vs Education", T1.summary , file="Cluster vs Education.txt",  sep = "\t" , fill = TRUE, append=TRUE) 

Table2 <- crosstab(audit.original.cluster, row.vars = "Cluster", col.vars = "Marital", type = c("f", "r"), style = "wide", 
                   addmargins = FALSE)
T2.summary <- capture.output(Table2)  
cat("Cluster vs Marital", T2.summary , file="Cluster vs Marital.txt",  sep = "\t" , fill = TRUE, append=TRUE) 

Table3 <- crosstab(audit.original.cluster, row.vars = "Cluster", col.vars = "Gender", type = c("f", "r"), style = "wide", 
                   addmargins = FALSE)
T3.summary <- capture.output(Table3)  
cat("Cluster vs Gender", T3.summary , file="Cluster vs Gender.txt",  sep = "\t" , fill = TRUE, append=TRUE) 

Table4 <- crosstab(audit.original.cluster, row.vars = "Cluster", col.vars = "Employment", type = c("f", "r"), style = "wide", 
                   addmargins = FALSE)
T4.summary <- capture.output(Table4)  
cat("Cluster vs Employment", T4.summary , file="Cluster vs Employment.txt",  sep = "\t" , fill = TRUE, append=TRUE) 

Table5 <- crosstab(audit.original.cluster, row.vars = "Cluster", col.vars = "Occupation", type = c("f", "r"), style = "wide", 
                   addmargins = FALSE)
T5.summary <- capture.output(Table5)  
cat("Cluster vs Occupation", T5.summary , file="Cluster vs Occupation.txt",  sep = "\t" , fill = TRUE, append=TRUE) 

Table6 <- crosstab(audit.original.cluster, row.vars = "Cluster", col.vars = "TARGET_Adjusted", type = c("f", "r"), style = "wide", 
                   addmargins = FALSE)
T6.summary <- capture.output(Table6)  
cat("Cluster vs TARGET_Adjusted", T6.summary , file="Cluster vs TARGET_Adjusted.txt",  sep = "\t" , fill = TRUE, append=TRUE) 

# Investigation between education and income
audit.raw <- audit <- read.csv("~/Desktop/GRADUATE/5703/final project/Data/audit.csv", stringsAsFactors=FALSE)

# Make ID as column ID
rownames(audit.raw) <- audit.raw$ID
audit.raw$ID <- NULL
table(audit.raw$Education, exclude = NULL)

selc.var <-c( "Income" , "Education")
audit.raw.edu.income <- audit.raw[, names(audit.raw) %in% selc.var, drop = F]

mean.audit.edu.income <- audit.raw.edu.income  %>%
  group_by(Education)  %>%
  summarise_all(funs(mean)) 
write.csv(mean.audit.edu.income, file = "Average income for different education levels.csv")


###############################################################
#delete employment

#scale
max <- apply(audit[,c("Age","Income","Deductions","Hours")],2,max)
min <- apply(audit[,c("Age","Income","Deductions","Hours")],2,min)
max
min
audit_sc <- as.data.frame(scale(audit[,c("Age","Income","Deductions","Hours")],center=min, scale=max-min))
summary(audit_sc)
setDT(audit_sc, keep.rownames = TRUE)[]
audit_c<-audit[,c(3:5,7,11)]
setDT(audit_c, keep.rownames = TRUE)[]
audit_sca<-merge(audit_sc,audit_c,by="rn")
#delete rn
audit_sca$rn<-NULL
library(dplyr)
audit_sca=audit_sca %>% mutate_if(is.character,as.factor)



#split data into training set and test set by 4:1
set.seed(123)
splitindex<-createDataPartition(audit_sca$TARGET_Adjusted,p=0.8,list=FALSE,times=1)
train<-audit_sca[splitindex,]
test<-audit_sca[-splitindex,]
table(train$TARGET_Adjusted)
table(test$TARGET_Adjusted)
###### model fitting-logistic regression #####
install.packages("caret")
library(caret) #to use createdatapartition

#logistic
logistic<-glm(TARGET_Adjusted~.,data=train,family=binomial)
summary(logistic)
#confusion matrix
prob<-predict(logistic,test,type='response')
pred<-rep('0',length(prob))
pred[prob>=0.5]<-'1'
tblog<-table(pred,test$TARGET_Adjusted)
tblog
rownames(tblog) <- paste("Pred", rownames(tblog), sep = ":")
colnames(tblog) <- paste("Actual", colnames(tblog), sep = ":")
print(tblog)

#prediction result has an accuracy of 83.75%, and a misclassification rate of 16.25%

############## support vector machine ##########
install.packages("e1071")
library(e1071)

#radial basis kernel
svm1 <- svm(TARGET_Adjusted ~ ., data = train)
summary(svm1)

svm.pred1 <- predict(svm1, test, type = 'response')
pred.svm1<-rep('0',length(svm.pred1))
pred.svm1[svm.pred1>=0.5]<-'1'
# confusion matrix 
tbsvm1 <- table(pred.svm1, test$TARGET_Adjusted)
tbsvm1
rownames(tbsvm1) <- paste("Pred", rownames(tbsvm1), sep = ":")
colnames(tbsvm1) <- paste("Actual", colnames(tbsvm1), sep = ":")
print(tbsvm1)
#prediction result has an accuracy of 85%, and a misclassification rate of 15%


#polynomial kernel
svm2 <- svm(TARGET_Adjusted ~ ., data = train,kernel="polynomial")
summary(svm2)
svm.pred2 <- predict(svm2, test, type = 'response')
pred.svm2<-rep('0',length(svm.pred2))
pred.svm2[svm.pred2>=0.5]<-'1'
# confusion matrix 
tbsvm2 <- table(pred.svm2, test$TARGET_Adjusted)
tbsvm2
rownames(tbsvm2) <- paste("Pred", rownames(tbsvm2), sep = ":")
colnames(tbsvm2) <- paste("Actual", colnames(tbsvm2), sep = ":")
print(tbsvm2)
#prediction result has an accuracy of 80.5%, and a misclassification rate of 19.5%


############# random forest ########
install.packages("randomForest")
install.packages("randomForestExplainer")
library(randomForest)
library(randomForestExplainer)
# Random forest
set.seed(123) 
rf <- randomForest(TARGET_Adjusted~.,data=train)
rf
prerf <- predict(rf,test,type='response')
pred.rf<-rep('0',length(prerf))
pred.rf[prerf>=0.5]<-'1'
# confusion matrix 
tbrf <- table(pred.rf, test$TARGET_Adjusted)
tbrf
rownames(tbrf) <- paste("Pred", rownames(tbrf), sep = ":")
colnames(tbrf) <- paste("Actual", colnames(tbrf), sep = ":")
print(tbrf)
importance(rf)
varImpPlot(rf)
#prediction result has an accuracy of 85%, and a misclassification rate of 15%
# Distribution of minimal depth
min_depth_frame <- min_depth_distribution(rf)
plot_min_depth_distribution(min_depth_frame)

# Neural Network
install.packages("neuralnet")
library(neuralnet)
m_train <- model.matrix(~., data=train)
m_test <- model.matrix(~.,data=test)
n <- colnames(m_train)
f <- as.formula(paste("TARGET_Adjusted ~", paste(n[!n %in% c("TARGET_Adjusted","(Intercept)")],
                                                  collapse = "+")))

# Compare the results of different hidden layers(from 1 to 10) using "nn$result.matrix" to choose the best one
set.seed(123)
nn1 <- neuralnet(f,data=m_train,hidden=1,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 569.38
nn1$result.matrix[1,]
set.seed(123)
nn2 <- neuralnet(f,data=m_train,hidden=2,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 525.39
nn2$result.matrix[1,]
set.seed(123)
nn3 <- neuralnet(f,data=m_train,hidden=3,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 531.39
nn3$result.matrix[1,]
set.seed(123)
nn4 <- neuralnet(f,data=m_train,hidden=4,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 495.73
nn4$result.matrix[1,]
set.seed(123)
nn5 <- neuralnet(f,data=m_train,hidden=5,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 461.53
nn5$result.matrix[1,]
set.seed(123)
nn6 <- neuralnet(f,data=m_train,hidden=6,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 434.71
nn6$result.matrix[1,]
set.seed(123)
nn7 <- neuralnet(f,data=m_train,hidden=7,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 398.31
nn7$result.matrix[1,]
set.seed(123)
nn8 <- neuralnet(f,data=m_train,hidden=8,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 374.33
nn8$result.matrix[1,]
set.seed(123)
nn9 <- neuralnet(f,data=m_train,hidden=9,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 378.91
nn9$result.matrix[1,]
set.seed(123)
nn10 <- neuralnet(f,data=m_train,hidden=10,err.fct="ce",linear.output=FALSE,stepmax=1e7) # error 343.61
nn10$result.matrix[1,]

# The one with the smallest error is 10 layers
plot(nn10)

# Accuracy on train set
output1 <- compute(nn10,m_train[,2:17])
p1 <- output1$net.result
head(p1)

pred1 <- ifelse(p1>0.5,1,0)
tab1 <- table(pred1,m_train[,18])
tab1
# Calculate the misclassification
MSE.nn1 <- sum((m_train[,18]-p1)^2)/nrow(m_train)
MSE.nn1

# Accuracy on test set
output2 <- compute(nn10,m_test[,2:17])
p2 <- output2$net.result
head(p2)
pred2 <- ifelse(p2>0.5,1,0)
tab2 <- table(pred2,m_test[,18])
tab2
MSE.nn2 <- sum((m_test[,18]-p2)^2)/nrow(m_test)
MSE.nn2

# According to result of the misclassification, choose 10 layers
